
@article{cuskley_limitations_2024,
	title = {The {Limitations} of {Large} {Language} {Models} for {Understanding} {Human} {Language} and {Cognition}},
	volume = {8},
	issn = {2470-2986},
	url = {https://direct.mit.edu/opmi/article/doi/10.1162/opmi_a_00160/124234/The-Limitations-of-Large-Language-Models-for},
	doi = {10.1162/opmi_a_00160},
	abstract = {Abstract
            Researchers have recently argued that the capabilities of Large Language Models (LLMs) can provide new insights into longstanding debates about the role of learning and/or innateness in the development and evolution of human language. Here, we argue on two grounds that LLMs alone tell us very little about human language and cognition in terms of acquisition and evolution. First, any similarities between human language and the output of LLMs are purely functional. Borrowing the “four questions” framework from ethology, we argue that what LLMs do is superficially similar, but how they do it is not. In contrast to the rich multimodal data humans leverage in interactive language learning, LLMs rely on immersive exposure to vastly greater quantities of unimodal text data, with recent multimodal efforts built upon mappings between images and text. Second, turning to functional similarities between human language and LLM output, we show that human linguistic behavior is much broader. LLMs were designed to imitate the very specific behavior of human writing; while they do this impressively, the underlying mechanisms of these models limit their capacities for meaning and naturalistic interaction, and their potential for dealing with the diversity in human language. We conclude by emphasising that LLMs are not theories of language, but tools that may be used to study language, and that can only be effectively applied with specific hypotheses to motivate research.},
	language = {en},
	urldate = {2025-11-24},
	journal = {Open Mind},
	author = {Cuskley, Christine and Woods, Rebecca and Flaherty, Molly},
	month = aug,
	year = {2024},
	pages = {1058--1083},
	file = {PDF:/Users/phantoms/Zotero/storage/VUSC45UF/Cuskley 等 - 2024 - The Limitations of Large Language Models for Understanding Human Language and Cognition.pdf:application/pdf},
}

@misc{liu_advances_2025,
	title = {Advances and {Challenges} in {Foundation} {Agents}: {From} {Brain}-{Inspired} {Intelligence} to {Evolutionary}, {Collaborative}, and {Safe} {Systems}},
	copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International},
	shorttitle = {Advances and {Challenges} in {Foundation} {Agents}},
	url = {https://arxiv.org/abs/2504.01990},
	doi = {10.48550/ARXIV.2504.01990},
	abstract = {The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This book provides a comprehensive overview, framing intelligent agents within modular, brain-inspired architectures that integrate principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we systematically investigate the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities and elucidating core components such as memory, world modeling, reward processing, goal, and emotion. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms. Third, we examine multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures. Finally, we address the critical imperative of building safe and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment. By synthesizing modular AI architectures with insights from different disciplines, this survey identifies key research challenges and opportunities, encouraging innovations that harmonize technological advancement with meaningful societal benefit.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Liu, Bang and Li, Xinfeng and Zhang, Jiayi and Wang, Jinlin and He, Tanjin and Hong, Sirui and Liu, Hongzhang and Zhang, Shaokun and Song, Kaitao and Zhu, Kunlun and Cheng, Yuheng and Wang, Suyuchen and Wang, Xiaoqiang and Luo, Yuyu and Jin, Haibo and Zhang, Peiyan and Liu, Ollie and Chen, Jiaqi and Zhang, Huan and Yu, Zhaoyang and Shi, Haochen and Li, Boyan and Wu, Dekun and Teng, Fengwei and Jia, Xiaojun and Xu, Jiawei and Xiang, Jinyu and Lin, Yizhang and Liu, Tianming and Liu, Tongliang and Su, Yu and Sun, Huan and Berseth, Glen and Nie, Jianyun and Foster, Ian and Ward, Logan and Wu, Qingyun and Gu, Yu and Zhuge, Mingchen and Liang, Xinbing and Tang, Xiangru and Wang, Haohan and You, Jiaxuan and Wang, Chi and Pei, Jian and Yang, Qiang and Qi, Xiaoliang and Wu, Chenglin},
	year = {2025},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), FOS: Computer and information sciences},
	file = {PDF:/Users/phantoms/Zotero/storage/B59THE9Y/Liu 等 - 2025 - Advances and Challenges in Foundation Agents From Brain-Inspired Intelligence to Evolutionary, Coll.pdf:application/pdf},
}

@article{griot_large_2025,
	title = {Large {Language} {Models} lack essential metacognition for reliable medical reasoning},
	volume = {16},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-024-55628-6},
	doi = {10.1038/s41467-024-55628-6},
	language = {en},
	number = {1},
	urldate = {2025-11-24},
	journal = {Nature Communications},
	author = {Griot, Maxime and Hemptinne, Coralie and Vanderdonckt, Jean and Yuksel, Demet},
	month = jan,
	year = {2025},
	pages = {642},
	file = {Full Text PDF:/Users/phantoms/Zotero/storage/HZMCBTH8/Griot 等 - 2025 - Large Language Models lack essential metacognition for reliable medical reasoning.pdf:application/pdf},
}

@misc{chhikara_mind_2025,
	title = {Mind the {Confidence} {Gap}: {Overconfidence}, {Calibration}, and {Distractor} {Effects} in {Large} {Language} {Models}},
	shorttitle = {Mind the {Confidence} {Gap}},
	url = {http://arxiv.org/abs/2502.11028},
	doi = {10.48550/arXiv.2502.11028},
	abstract = {Large Language Models (LLMs) show remarkable proficiency in natural language tasks, yet their frequent overconfidence-misalignment between predicted confidence and true correctness-poses significant risks in critical decision-making applications. We present a comprehensive analysis on calibration in LLMs across nine LLMs and three factual Question-Answering (QA) datasets, systematically comparing standard free-generation settings against structured distractor-augmented prompts. Our evaluation reveals that explicitly incorporating distractors can substantially mitigate miscalibration, achieving relative accuracy improvements up to 460\% and ECE reductions up to 90\%. Despite general trends, we uncover nuanced findings: large RLHF-tuned models display inherent calibration strengths but can paradoxically suffer increased miscalibration on easier queries, whereas smaller models benefit disproportionately from distractor prompts but remain significantly miscalibrated. Through detailed analyses across question types, we identify persistent calibration failures, particularly in person-based queries. We conclude with concrete recommendations-targeted fine-tuning, structured prompting, and strategic model choice-to ensure reliable, trustworthy LLM deployments.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Chhikara, Prateek},
	month = jun,
	year = {2025},
	note = {arXiv:2502.11028 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/phantoms/Zotero/storage/WF2VXDZR/Chhikara - 2025 - Mind the Confidence Gap Overconfidence, Calibration, and Distractor Effects in Large Language Model.pdf:application/pdf;Snapshot:/Users/phantoms/Zotero/storage/BA346YFB/2502.html:text/html},
}

@misc{pan_automatically_2023,
	title = {Automatically {Correcting} {Large} {Language} {Models}: {Surveying} the landscape of diverse self-correction strategies},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Automatically {Correcting} {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2308.03188},
	doi = {10.48550/ARXIV.2308.03188},
	abstract = {Large language models (LLMs) have demonstrated remarkable performance across a wide array of NLP tasks. However, their efficacy is undermined by undesired and inconsistent behaviors, including hallucination, unfaithful reasoning, and toxic content. A promising approach to rectify these flaws is self-correction, where the LLM itself is prompted or guided to fix problems in its own output. Techniques leveraging automated feedback -- either produced by the LLM itself or some external system -- are of particular interest as they are a promising way to make LLM-based solutions more practical and deployable with minimal human feedback. This paper presents a comprehensive review of this emerging class of techniques. We analyze and taxonomize a wide array of recent work utilizing these strategies, including training-time, generation-time, and post-hoc correction. We also summarize the major applications of this strategy and conclude by discussing future directions and challenges.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Pan, Liangming and Saxon, Michael and Xu, Wenda and Nathani, Deepak and Wang, Xinyi and Wang, William Yang},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences, Machine Learning (cs.LG)},
	file = {PDF:/Users/phantoms/Zotero/storage/3LZ23GB2/Pan 等 - 2023 - Automatically Correcting Large Language Models Surveying the landscape of diverse self-correction s.pdf:application/pdf},
}

@misc{zhou_context-faithful_2023,
	title = {Context-faithful {Prompting} for {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2303.11315},
	doi = {10.48550/ARXIV.2303.11315},
	abstract = {Large language models (LLMs) encode parametric knowledge about world facts and have shown remarkable performance in knowledge-driven NLP tasks. However, their reliance on parametric knowledge may cause them to overlook contextual cues, leading to incorrect predictions in context-sensitive NLP tasks (e.g., knowledge acquisition tasks). In this paper, we seek to assess and enhance LLMs' contextual faithfulness in two aspects: knowledge conflict and prediction with abstention. We demonstrate that LLMs' faithfulness can be significantly improved using carefully designed prompting strategies. In particular, we identify opinion-based prompts and counterfactual demonstrations as the most effective methods. Opinion-based prompts reframe the context as a narrator's statement and inquire about the narrator's opinions, while counterfactual demonstrations use instances containing false facts to improve faithfulness in knowledge conflict situations. Neither technique requires additional training. We conduct experiments on three datasets of two standard NLP tasks, machine reading comprehension and relation extraction, and the results demonstrate significant improvement in faithfulness to contexts. Code and data are released at https://github.com/wzhouad/context-faithful-llm.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Zhou, Wenxuan and Zhang, Sheng and Poon, Hoifung and Chen, Muhao},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Computation and Language (cs.CL), FOS: Computer and information sciences},
	file = {PDF:/Users/phantoms/Zotero/storage/F593UBS6/Zhou 等 - 2023 - Context-faithful Prompting for Large Language Models.pdf:application/pdf},
}

@misc{ji_towards_2023,
	title = {Towards {Mitigating} {Hallucination} in {Large} {Language} {Models} via {Self}-{Reflection}},
	copyright = {Creative Commons Attribution 4.0 International},
	url = {https://arxiv.org/abs/2310.06271},
	doi = {10.48550/ARXIV.2310.06271},
	abstract = {Large language models (LLMs) have shown promise for generative and knowledge-intensive tasks including question-answering (QA) tasks. However, the practical deployment still faces challenges, notably the issue of "hallucination", where models generate plausible-sounding but unfaithful or nonsensical information. This issue becomes particularly critical in the medical domain due to the uncommon professional concepts and potential social risks involved. This paper analyses the phenomenon of hallucination in medical generative QA systems using widely adopted LLMs and datasets. Our investigation centers on the identification and comprehension of common problematic answers, with a specific emphasis on hallucination. To tackle this challenge, we present an interactive self-reflection methodology that incorporates knowledge acquisition and answer generation. Through this feedback process, our approach steadily enhances the factuality, consistency, and entailment of the generated answers. Consequently, we harness the interactivity and multitasking ability of LLMs and produce progressively more precise and accurate answers. Experimental results on both automatic and human evaluation demonstrate the superiority of our approach in hallucination reduction compared to baselines.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Ji, Ziwei and Yu, Tiezheng and Xu, Yan and Lee, Nayeon and Ishii, Etsuko and Fung, Pascale},
	year = {2023},
	note = {Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
	file = {PDF:/Users/phantoms/Zotero/storage/LCH2KXTU/Ji 等 - 2023 - Towards Mitigating Hallucination in Large Language Models via Self-Reflection.pdf:application/pdf},
}

@inproceedings{zhou_metacognitive_2024,
	address = {Singapore Singapore},
	title = {Metacognitive {Retrieval}-{Augmented} {Large} {Language} {Models}},
	isbn = {979-8-4007-0171-9},
	url = {https://dl.acm.org/doi/10.1145/3589334.3645481},
	doi = {10.1145/3589334.3645481},
	language = {en},
	urldate = {2025-11-24},
	booktitle = {Proceedings of the {ACM} {Web} {Conference} 2024},
	publisher = {ACM},
	author = {Zhou, Yujia and Liu, Zheng and Jin, Jiajie and Nie, Jian-Yun and Dou, Zhicheng},
	month = may,
	year = {2024},
	pages = {1453--1463},
	file = {PDF:/Users/phantoms/Zotero/storage/HKQARTDK/Zhou 等 - 2024 - Metacognitive Retrieval-Augmented Large Language Models.pdf:application/pdf},
}

@misc{dhuliawala_chain--verification_2023,
	title = {Chain-of-{Verification} {Reduces} {Hallucination} in {Large} {Language} {Models}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/2309.11495},
	doi = {10.48550/ARXIV.2309.11495},
	abstract = {Generation of plausible yet incorrect factual information, termed hallucination, is an unsolved issue in large language models. We study the ability of language models to deliberate on the responses they give in order to correct their mistakes. We develop the Chain-of-Verification (CoVe) method whereby the model first (i) drafts an initial response; then (ii) plans verification questions to fact-check its draft; (iii) answers those questions independently so the answers are not biased by other responses; and (iv) generates its final verified response. In experiments, we show CoVe decreases hallucinations across a variety of tasks, from list-based questions from Wikidata, closed book MultiSpanQA and longform text generation.},
	urldate = {2025-11-24},
	publisher = {arXiv},
	author = {Dhuliawala, Shehzaad and Komeili, Mojtaba and Xu, Jing and Raileanu, Roberta and Li, Xian and Celikyilmaz, Asli and Weston, Jason},
	year = {2023},
	note = {Version Number: 2},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
	file = {PDF:/Users/phantoms/Zotero/storage/W97MFGUM/Dhuliawala 等 - 2023 - Chain-of-Verification Reduces Hallucination in Large Language Models.pdf:application/pdf},
}

@article{scaria_automated_2024,
	title = {Automated {Educational} {Question} {Generation} at {Different} {Bloom}'s {Skill} {Levels} using {Large} {Language} {Models}: {Strategies} and {Evaluation}},
	copyright = {Creative Commons Attribution 4.0 International},
	shorttitle = {Automated {Educational} {Question} {Generation} at {Different} {Bloom}'s {Skill} {Levels} using {Large} {Language} {Models}},
	url = {https://arxiv.org/abs/2408.04394},
	doi = {10.48550/ARXIV.2408.04394},
	abstract = {Developing questions that are pedagogically sound, relevant, and promote learning is a challenging and time-consuming task for educators. Modern-day large language models (LLMs) generate high-quality content across multiple domains, potentially helping educators to develop high-quality questions. Automated educational question generation (AEQG) is important in scaling online education catering to a diverse student population. Past attempts at AEQG have shown limited abilities to generate questions at higher cognitive levels. In this study, we examine the ability of five state-of-the-art LLMs of different sizes to generate diverse and high-quality questions of different cognitive levels, as defined by Bloom's taxonomy. We use advanced prompting techniques with varying complexity for AEQG. We conducted expert and LLM-based evaluations to assess the linguistic and pedagogical relevance and quality of the questions. Our findings suggest that LLms can generate relevant and high-quality educational questions of different cognitive levels when prompted with adequate information, although there is a significant variance in the performance of the five LLms considered. We also show that automated evaluation is not on par with human evaluation.},
	urldate = {2025-11-25},
	author = {Scaria, Nicy and Chenna, Suma Dharani and Subramani, Deepak},
	year = {2024},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Artificial Intelligence (cs.AI), Computation and Language (cs.CL), FOS: Computer and information sciences},
	file = {PDF:/Users/phantoms/Zotero/storage/5MFAIF38/Scaria 等 - 2024 - Automated Educational Question Generation at Different Bloom's Skill Levels using Large Language Mod.pdf:application/pdf},
}

@article{gui_effectiveness_2023,
	title = {Effectiveness of digital educational game and game design in {STEM} learning: a meta-analytic review},
	volume = {10},
	issn = {2196-7822},
	shorttitle = {Effectiveness of digital educational game and game design in {STEM} learning},
	url = {https://stemeducationjournal.springeropen.com/articles/10.1186/s40594-023-00424-9},
	doi = {10.1186/s40594-023-00424-9},
	abstract = {Abstract
            
              Digital educational games exhibit substantial promise in advancing STEM education. Nevertheless, the empirical evidence on both the efficacy of digital game-based learning and its designs in STEM education is characterized by notable inconsistencies. Therefore, the current study aimed to investigate (1) the general effect of digital game-based STEM learning over STEM learning without digital game, and (2) the enhancement effect of added game-design elements against base game versions in STEM learning. Two meta-analyses were conducted in this study. Based on the 136 effect sizes extracted from 86 studies, the first meta-analysis revealed a medium to large general effect of digital game-based STEM learning over conventional STEM learning (
              g
               = 0.624, 95\% CI [0.457, 0.790]). In addition, digital game-based STEM learning appeared to be differentially effective for different learning outcome, different types of game, and different subject. A total of 44 primary studies and 81 effect sizes were identified in the second meta-analysis. The results revealed a small to medium enhancement effect of added game-design elements over base game versions (
              g
               = 0.301, 95\% CI [0.163, 0.438]). Furthermore, our results indicated that the game-design elements added for content learning were more effective than those added for gaming experience. Possible explanations for these findings, as well as the limitations and directions for future research were discussed.},
	language = {en},
	number = {1},
	urldate = {2025-11-26},
	journal = {International Journal of STEM Education},
	author = {Gui, Yang and Cai, Zhihui and Yang, Yajiao and Kong, Lingyuan and Fan, Xitao and Tai, Robert H.},
	month = may,
	year = {2023},
	pages = {36},
	file = {PDF:/Users/phantoms/Zotero/storage/IPHBY9QF/Gui 等 - 2023 - Effectiveness of digital educational game and game design in STEM learning a meta-analytic review.pdf:application/pdf},
}

@article{koupritzioti_pydiophantus_2020,
	title = {{PyDiophantus} maze game: {Play} it to learn mathematics or implement it to learn game programming in {Python}},
	volume = {25},
	issn = {1360-2357, 1573-7608},
	shorttitle = {{PyDiophantus} maze game},
	url = {http://link.springer.com/10.1007/s10639-019-10087-1},
	doi = {10.1007/s10639-019-10087-1},
	language = {en},
	number = {4},
	urldate = {2025-11-15},
	journal = {Education and Information Technologies},
	author = {Koupritzioti, Dimitra and Xinogalos, Stelios},
	month = jul,
	year = {2020},
	pages = {2747--2764},
	file = {PDF:/Users/phantoms/Zotero/storage/23G9E8NQ/Koupritzioti和Xinogalos - 2020 - PyDiophantus maze game Play it to learn mathematics or implement it to learn game programming in Py.pdf:application/pdf},
}

@article{goslen_llm-based_2025,
	title = {{LLM}-{Based} {Student} {Plan} {Generation} for {Adaptive} {Scaffolding} in {Game}-{Based} {Learning} {Environments}},
	volume = {35},
	issn = {1560-4292, 1560-4306},
	url = {https://link.springer.com/10.1007/s40593-024-00421-1},
	doi = {10.1007/s40593-024-00421-1},
	language = {en},
	number = {2},
	urldate = {2025-11-16},
	journal = {International Journal of Artificial Intelligence in Education},
	author = {Goslen, Alex and Kim, Yeo Jin and Rowe, Jonathan and Lester, James},
	month = jun,
	year = {2025},
	pages = {533--558},
	file = {PDF:/Users/phantoms/Zotero/storage/UAKVBINP/Goslen 等 - 2025 - LLM-Based Student Plan Generation for Adaptive Scaffolding in Game-Based Learning Environments.pdf:application/pdf},
}

@article{dominguez_gamifying_2013,
	title = {Gamifying learning experiences: {Practical} implications and outcomes},
	volume = {63},
	copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
	issn = {03601315},
	shorttitle = {Gamifying learning experiences},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0360131513000031},
	doi = {10.1016/j.compedu.2012.12.020},
	language = {en},
	urldate = {2025-11-16},
	journal = {Computers \& Education},
	author = {Domínguez, Adrián and Saenz-de-Navarrete, Joseba and de-Marcos, Luis and Fernández-Sanz, Luis and Pagés, Carmen and Martínez-Herráiz, José-Javier},
	month = apr,
	year = {2013},
	pages = {380--392},
	file = {PDF:/Users/phantoms/Zotero/storage/V78PWMX8/Domínguez 等 - 2013 - Gamifying learning experiences Practical implications and outcomes.pdf:application/pdf},
}

@incollection{boekaerts_emotions_2015,
	title = {Emotions and emotion regulation in academic settings},
	booktitle = {Handbook of educational psychology},
	publisher = {Routledge},
	author = {Boekaerts, Monique and Pekrun, Reinhard},
	year = {2015},
	pages = {90--104},
}

@misc{jiang_httpsgithubcomshaobin-jiangcourse-game_2023,
	title = {https://github.com/{Shaobin}-{Jiang}/course-game},
	url = {https://github.com/Shaobin-Jiang/course-game},
	publisher = {GitHub},
	author = {Jiang, S.},
	year = {2023},
	note = {Publication Title: GitHub repository},
}

@article{bernard_examining_nodate,
	title = {Examining the {Impact} of {Game}-{Based} {Learning} on {Student} {Engagement} and {Performance} in an {Introductory} {Computer} {Programming} {Course} at the {University} of the {Southern} {Caribbean} ({USC}).},
    year = {2022},
	abstract = {Background: At the University of the Southern Caribbean (USC) students often struggle with learning programming. Because of this struggle, they often become disengaged with the programming courses, with some transferring to other degree programmes or withdrawing from the programme. While several strategies have been used to ensure that students can problemsolve, design, and develop coded solutions, it has not been enough to alleviate the issues. Gamebased learning (GBL) emerged as a possible strategy that can potentially help students develop these skills while keeping them engaged with the course content. Aim: Implementing such a strategy within the department requires evidence that it can be an effective technique for teaching and learning programming. Therefore, the aim of this study is to evaluate the impact of GBL on student engagement and overall performance in an introductory programming course. Method: The research was designed as a deductive exploratory single case study research strategy and method. It approaches the aims and objectives from a pragmatic perspective, and as a result, uses a mixed methodological approach to data collection and analysis. Findings: The findings show that while GBL does not alleviate the common negative reactions to learning programming, it does provide a learning environment engaging enough for students to overlook these. This results in students having an enhanced perception of the knowledge and improved performance.},
	language = {en},
	author = {Bernard, Fayola St},
	file = {PDF:/Users/phantoms/Zotero/storage/BRRG8FGI/Bernard - Examining the Impact of Game-Based Learning on Student Engagement and Performance in an Introductory.pdf:application/pdf},
}

@article{de_smet_qualitative_2024,
	title = {A qualitative evaluation study of introducing game-based learning methods during pre-service teachers’ {Internship}},
	volume = {83},
	issn = {0191491X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0191491X24000671},
	doi = {10.1016/j.stueduc.2024.101388},
	language = {en},
	urldate = {2025-11-26},
	journal = {Studies in Educational Evaluation},
	author = {De Smet, Cindy},
	month = dec,
	year = {2024},
	pages = {101388},
}

@misc{sobke_sewer_2018,
	title = {Sewer {Rats} in {Teaching} {Action}: {An} explorative field study on students' perception of a game-based learning app in graduate engineering education},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {Sewer {Rats} in {Teaching} {Action}},
	url = {https://arxiv.org/abs/1811.09776},
	doi = {10.48550/ARXIV.1811.09776},
	abstract = {Game-based technologies and mobile learning aids open up many opportunities for learners; however, evidence-based decisions on their appropriate use are necessary. This explorative study (N = 100) examines the role of game elements in university education using a game-based learning app for mobile devices. The educational goal of the app is to support students in the field of engineering to memorize factual knowledge. The study investigates how the game-based app affects learners' motivation. It analyses the perceived impact and appeal as well as the game elements as an incentive in learners' perception. To realize this aim, the study combines structured methods like questionnaires with semi-structured methods like thinking aloud, game diaries, and interviews. The results indicate that flexible tem-poral and spatial use of the app was an important factor of learners' motivation. The app allowed more spontaneous involvement with the subject matter and the learners took advantage of an improved attitude toward the subject matter. However, only a low impact on intrinsic motivation could be observed. We discuss reasons and present practical implications.},
	urldate = {2025-11-26},
	publisher = {arXiv},
	author = {Söbke, Heinrich and Reichelt, Maria},
	year = {2018},
	note = {Version Number: 1},
	keywords = {Computers and Society (cs.CY), FOS: Computer and information sciences, Human-Computer Interaction (cs.HC), Multimedia (cs.MM)},
}

@misc{elkins_how_2025,
	title = {How {Teachers} {Can} {Use} {Large} {Language} {Models} and {Bloom}'s {Taxonomy} to {Create} {Educational} {Quizzes}},
	url = {http://arxiv.org/abs/2401.05914},
	doi = {10.48550/arXiv.2401.05914},
	abstract = {Question generation (QG) is a natural language processing task with an abundance of potential benefits and use cases in the educational domain. In order for this potential to be realized, QG systems must be designed and validated with pedagogical needs in mind. However, little research has assessed or designed QG approaches with the input from real teachers or students. This paper applies a large language model-based QG approach where questions are generated with learning goals derived from Bloom's taxonomy. The automatically generated questions are used in multiple experiments designed to assess how teachers use them in practice. The results demonstrate that teachers prefer to write quizzes with automatically generated questions, and that such quizzes have no loss in quality compared to handwritten versions. Further, several metrics indicate that automatically generated questions can even improve the quality of the quizzes created, showing the promise for large scale use of QG in the classroom setting.},
	urldate = {2025-11-27},
	publisher = {arXiv},
	author = {Elkins, Sabina and Kochmar, Ekaterina and Cheung, Jackie C. K. and Serban, Iulian},
	month = nov,
	year = {2025},
	note = {arXiv:2401.05914 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Preprint PDF:/Users/phantoms/Zotero/storage/6ILBIKIM/Elkins 等 - 2025 - How Teachers Can Use Large Language Models and Bloom's Taxonomy to Create Educational Quizzes.pdf:application/pdf;Snapshot:/Users/phantoms/Zotero/storage/DYAV7P7G/2401.html:text/html},
}

@incollection{ferreira_mello_bloomllm_2024,
	address = {Cham},
	title = {{BloomLLM}: {Large} {Language} {Models} {Based} {Question} {Generation} {Combining} {Supervised} {Fine}-{Tuning} and {Bloom}’s {Taxonomy}},
	volume = {15160},
	isbn = {978-3-031-72311-7 978-3-031-72312-4},
	shorttitle = {{BloomLLM}},
	url = {https://link.springer.com/10.1007/978-3-031-72312-4_11},
	language = {en},
	urldate = {2025-11-27},
	booktitle = {Technology {Enhanced} {Learning} for {Inclusive} and {Equitable} {Quality} {Education}},
	publisher = {Springer Nature Switzerland},
	author = {Duong-Trung, Nghia and Wang, Xia and Kravčík, Miloš},
	editor = {Ferreira Mello, Rafael and Rummel, Nikol and Jivet, Ioana and Pishtari, Gerti and Ruipérez Valiente, José A.},
	year = {2024},
	doi = {10.1007/978-3-031-72312-4_11},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {93--98},
}

@article{ryan_motivational_2006,
	title = {The {Motivational} {Pull} of {Video} {Games}: {A} {Self}-{Determination} {Theory} {Approach}},
	volume = {30},
	copyright = {http://www.springer.com/tdm},
	issn = {0146-7239, 1573-6644},
	shorttitle = {The {Motivational} {Pull} of {Video} {Games}},
	url = {http://link.springer.com/10.1007/s11031-006-9051-8},
	doi = {10.1007/s11031-006-9051-8},
	language = {en},
	number = {4},
	urldate = {2025-11-27},
	journal = {Motivation and Emotion},
	author = {Ryan, Richard M. and Rigby, C. Scott and Przybylski, Andrew},
	month = dec,
	year = {2006},
	pages = {344--360},
}

@article{tyack_self-determination_2024,
	title = {Self-{Determination} {Theory} and {HCI} {Games} {Research}: {Unfulfilled} {Promises} and {Unquestioned} {Paradigms}},
	volume = {31},
	issn = {1073-0516, 1557-7325},
	shorttitle = {Self-{Determination} {Theory} and {HCI} {Games} {Research}},
	url = {https://dl.acm.org/doi/10.1145/3673230},
	doi = {10.1145/3673230},
	abstract = {Self-determination theory (SDT), a psychological theory of human motivation, is a prominent paradigm in human–computer interaction (HCI) research on games. However, our prior literature review observed a trend towards shallow applications of the theory. This follow-up work takes a broader view—examining SDT scholarship on games, a wider corpus of SDT-based HCI games research (N = 259), and perspectives from a games industry practitioner conference—to help explain current applications of SDT. Our findings suggest that perfunctory applications of the theory in HCI games research originate in part from within SDT scholarship on games, which itself exhibits limited engagement with theoretical tenets. Against this backdrop, we unpack the popularity of SDT in HCI games research and identify conditions underlying the theory’s current use as an oft-unquestioned paradigm. Finally, we outline avenues for more productive SDT-informed games research and consider ways towards more intentional practices of theory use in HCI.},
	language = {en},
	number = {3},
	urldate = {2025-11-27},
	journal = {ACM Transactions on Computer-Human Interaction},
	author = {Tyack, April and Mekler, Elisa D.},
	month = jun,
	year = {2024},
	pages = {1--74},
	file = {全文:/Users/phantoms/Zotero/storage/VEEVXK8U/Tyack和Mekler - 2024 - Self-Determination Theory and HCI Games Research Unfulfilled Promises and Unquestioned Paradigms.pdf:application/pdf},
}
